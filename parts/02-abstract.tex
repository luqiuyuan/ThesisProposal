% A B S T R A C T
% ---------------

\begin{center}\textbf{Abstract}\end{center}

In the past two decades, augmented reality (AR) has received a growing amount of attention by researchers in the area of training, because AR can be applied to address a wide range of problems facing the traditional training tools.
This article proposes a novel training framework based on AR to facilitate training process.
The framework uses a client-server structure. The two types of clients (i.e. trainers and trainees) are connected through a central server.
Both of the trainers and the trainees use AR glasses or mobile devices to capture videos of the environment.
Given the 3D structures of the environment (components involved in training processes), we leverage a novel video object segmentation strategy that we created to track the 3D poses of the components without markers.
Moving objects are rendered and the frames are sent to the clients and overlaid on the real scenes, i.e. any objects moved by the trainer will be rendered and sent to the trainee, and, similarly, any objects moved by the trainee are rendered and sent to the trainer.
During this course, the remote trainer guides the trainees in a collaborative way. Therefore, the trainer is able to give customized feedback to each trainee.
Our goal is to work with devices without powerful graphics capacity. Tracking and a part of the rendering task are offloaded to the server to relieve the computing burden of the AR glasses and mobile devices.

Compared to the existing AR training applications, our proposed framework has three advantages:
1) It does not require a pre-defined guidance, which makes it suitable to complex tasks;
2) It allows the trainer to collect feedbacks from the trainees during the training process in real-time;
3) It works with mobile devices without powerful graphics capacity.

To realize the framework we propose, we have developed two methods: an object video segmentation algorithm and a hybrid remote rendering framework.

The first method is an object video segmentation algorithm used for extracting foreground objects from videos. Our method uses trimaps inferred from background subtraction to represent the foreground-background relationship. The appearance of foreground and background are modelled with Radial Basis Functions and we use graph cuts to compute a binary mask. Our method is fully automatic, fast, and does not make restrictive assumptions about object motions. In experiments on standard data sets, the proposed approach achieves comparable results to the state-of-the-art video object segmentation methods but our method is much faster.
With this algorithm, we are able to leverage an existing 3D object pose estimation algorithm to perform a markerless
% wording: it should be markerless instead of marker-less
3D structure estimation for the models and the environment in a training scenario.

The second method is a hybrid remote rendering framework for applications on mobile devices. In our remote rendering approach, we adopt a client-server model, where the server is responsible for rendering high-fidelity models, encoding the rendering results and sending them to the client, while the client renders low-fidelity models and overlays the high-fidelity frames received from the server on its rendering results. With this configuration, the client is able to keep functioning regardless of network condition and bandwidth. Moreover, to minimize the bandwidth requirements, only key models are rendered in high-fidelity mode.

The remaining work includes the implementation of the communication schema that manages the communications between the trainer and the trainees, and the integration of all parts into one framework.

\cleardoublepage
%\newpage
